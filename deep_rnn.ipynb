{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Возьмите англо-русскую пару фраз (https://www.manythings.org/anki/).\n",
        "2. Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество.\n",
        "3. Попробуйте добавить +1 рекуррентный слой в encoder и decoder.\n",
        "4. Попробуйте заменить GRU ячейки на lstm-ячейки Оцените качество во всех случаях."
      ],
      "metadata": {
        "id": "IMoA4Oxxo4bt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QqnNwc3Ko34E"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import product\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yHi-koFHpemr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/rus-eng.zip\n",
        "!unzip rus-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5aMaLvXpjLL",
        "outputId": "3e397999-15c3-4359-d07b-f36172613393"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-30 08:55:16--  https://www.manythings.org/anki/rus-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16305013 (16M) [application/zip]\n",
            "Saving to: ‘rus-eng.zip’\n",
            "\n",
            "rus-eng.zip         100%[===================>]  15.55M  18.8MB/s    in 0.8s    \n",
            "\n",
            "2025-01-30 08:55:17 (18.8 MB/s) - ‘rus-eng.zip’ saved [16305013/16305013]\n",
            "\n",
            "Archive:  rus-eng.zip\n",
            "  inflating: rus.txt                 \n",
            "  inflating: _about.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head rus.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Va4UdobpmPu",
        "outputId": "9addac0c-f5d0-44fb-f144-11111bfaa018"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tМарш!\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)\n",
            "Go.\tИди.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898247 (marafon)\n",
            "Go.\tИдите.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898250 (marafon)\n",
            "Hi.\tЗдравствуйте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #402127 (odexed)\n",
            "Hi.\tПривет!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #466968 (katjka)\n",
            "Hi.\tХай.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #467233 (timsa)\n",
            "Hi.\tЗдрасте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3803577 (marafon)\n",
            "Hi.\tЗдоро́во!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3854188 (marafon)\n",
            "Hi.\tПриветик!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #7234283 (marafon)\n",
            "Run!\tБеги!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #1569978 (Biga)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "lang1 = 'rus'\n",
        "lang2 = 'eng'\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "JI3dtnuypqHh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Zа-яА-Я.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "j-U-PcjeptI9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines    #data/%s-%s.txt\n",
        "    # lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "    lines = open('rus.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:-1]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang  = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "Hp4cEQrZqKJV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "mrUEMo-qbo47"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('rus', 'eng', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czz9S8Qrbsuo",
        "outputId": "fdf09475-0f1d-411a-948e-5ff5dfc2a7d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 496059 sentence pairs\n",
            "Trimmed to 28719 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 10177\n",
            "rus 4303\n",
            "['даю тебе последнии шанс .', 'i m giving you one last chance .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Encoder"
      ],
      "metadata": {
        "id": "TtqrzHAgfJBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers, rnn_type):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = rnn_type(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, n_layers):\n",
        "        return torch.zeros(n_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "IaeZd2foccL6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder"
      ],
      "metadata": {
        "id": "N6p-R3HRfPTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers, rnn_type):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.rnn = rnn_type(hidden_size, hidden_size, n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, n_layers):\n",
        "        return torch.zeros(n_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "xVRtFLrHfMwB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "quR_0tDXf3-p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, rnn_ty, n_layers, max_length=MAX_LENGTH):\n",
        "\n",
        "    if rnn_ty.__name__ == 'LSTM':\n",
        "        encoder_hidden = (encoder.initHidden(n_layers), encoder.initHidden(n_layers))\n",
        "    else:\n",
        "        encoder_hidden = encoder.initHidden(n_layers)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "RAruIbBvf6FK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "6pyx_4zmf-Bw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "k44o2j_qgIsK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, rnn_typ, nu_layers, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion, rnn_typ, nu_layers)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "CDpSa2DugA5C"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, n_layers, Rnn, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        if Rnn.__name__ == 'LSTM':\n",
        "            encoder_hidden = (encoder.initHidden(n_layers), encoder.initHidden(n_layers))\n",
        "        else:\n",
        "            encoder_hidden = encoder.initHidden(n_layers)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "Nwf9EIOogEoj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, _layers, rnn_, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0], _layers, rnn_)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "jkZvBNnlgMhH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 слой + GRU"
      ],
      "metadata": {
        "id": "l4XgOsNYuFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "rnn_type = nn.GRU #nn.LSTM, nn.GRU\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers, rnn_type).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers, rnn_type).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75000, rnn_type, num_layers, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv9RuvIZgO7e",
        "outputId": "b47ea810-5a4e-4dce-960d-9ae6ffb5922e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 15s (- 17m 33s) (5000 6%) 3.1449\n",
            "2m 27s (- 15m 57s) (10000 13%) 2.6563\n",
            "3m 40s (- 14m 40s) (15000 20%) 2.3745\n",
            "4m 51s (- 13m 20s) (20000 26%) 2.1816\n",
            "6m 3s (- 12m 7s) (25000 33%) 2.0401\n",
            "7m 16s (- 10m 54s) (30000 40%) 1.9030\n",
            "8m 27s (- 9m 40s) (35000 46%) 1.7830\n",
            "9m 40s (- 8m 27s) (40000 53%) 1.6821\n",
            "10m 52s (- 7m 14s) (45000 60%) 1.6021\n",
            "12m 4s (- 6m 2s) (50000 66%) 1.5190\n",
            "13m 17s (- 4m 49s) (55000 73%) 1.4584\n",
            "14m 28s (- 3m 37s) (60000 80%) 1.3765\n",
            "15m 41s (- 2m 24s) (65000 86%) 1.3180\n",
            "16m 53s (- 1m 12s) (70000 93%) 1.2881\n",
            "18m 6s (- 0m 0s) (75000 100%) 1.2043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, num_layers, rnn_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlfHLK9JgUYG",
        "outputId": "db87de5e-6ac8-4a01-d758-5b6edc8c6ec1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> на неи солнечные очки .\n",
            "= she s wearing sunglasses .\n",
            "< she s wearing a hat . <EOS>\n",
            "\n",
            "> вы такои сексуальныи .\n",
            "= you re so sexy .\n",
            "< you re so perfect . <EOS>\n",
            "\n",
            "> я у тебя помощи не прошу .\n",
            "= i m not asking for your help .\n",
            "< i m not asking for your help . <EOS>\n",
            "\n",
            "> мы остановились в другои гостинице .\n",
            "= we re staying in a different hotel .\n",
            "< we re working at the hotel . <EOS>\n",
            "\n",
            "> ты поранишься .\n",
            "= you are going to get hurt .\n",
            "< you re a . <EOS>\n",
            "\n",
            "> мы теперь семья .\n",
            "= we re family now .\n",
            "< we re a family now . <EOS>\n",
            "\n",
            "> ты дрожишь . замерз ?\n",
            "= you re shivering . are you cold ?\n",
            "< you re shivering right ? <EOS>\n",
            "\n",
            "> ты великодушна .\n",
            "= you re generous .\n",
            "< you re generous . <EOS>\n",
            "\n",
            "> он хорошо известен у нас в стране .\n",
            "= he is well known in our country .\n",
            "< he is in in a of of . <EOS>\n",
            "\n",
            "> ты самодовольна .\n",
            "= you re vain .\n",
            "< you re a . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 слоя + GRU"
      ],
      "metadata": {
        "id": "5uO09mushNq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "rnn_type = nn.GRU #nn.LSTM, nn.GRU\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers, rnn_type).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers, rnn_type).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75000, rnn_type, num_layers, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XkXBo8AyGTa",
        "outputId": "d765c8f3-0e49-4324-b24d-32700c3381a7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 39s (- 23m 12s) (5000 6%) 3.1412\n",
            "3m 16s (- 21m 16s) (10000 13%) 2.6938\n",
            "4m 52s (- 19m 30s) (15000 20%) 2.4589\n",
            "6m 29s (- 17m 51s) (20000 26%) 2.2498\n",
            "8m 3s (- 16m 6s) (25000 33%) 2.0775\n",
            "9m 39s (- 14m 29s) (30000 40%) 1.9774\n",
            "11m 13s (- 12m 49s) (35000 46%) 1.8524\n",
            "12m 47s (- 11m 11s) (40000 53%) 1.7408\n",
            "14m 22s (- 9m 34s) (45000 60%) 1.6199\n",
            "15m 56s (- 7m 58s) (50000 66%) 1.5283\n",
            "17m 31s (- 6m 22s) (55000 73%) 1.4842\n",
            "19m 6s (- 4m 46s) (60000 80%) 1.3690\n",
            "20m 41s (- 3m 10s) (65000 86%) 1.3135\n",
            "22m 15s (- 1m 35s) (70000 93%) 1.2822\n",
            "23m 50s (- 0m 0s) (75000 100%) 1.2130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, num_layers, rnn_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYSra3-MyJ0q",
        "outputId": "6f3a85da-7a65-44f9-e9f8-1b2f2a845a0e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> я теперь себя чувствую очень виноватои .\n",
            "= i m feeling very guilty now .\n",
            "< i m feeling very good . <EOS>\n",
            "\n",
            "> мои рост метр пятьдесят восемь .\n",
            "= i am five feet two inches tall .\n",
            "< i am five two two tall tall . <EOS>\n",
            "\n",
            "> я сеичас один .\n",
            "= i m alone now .\n",
            "< i m alone now . <EOS>\n",
            "\n",
            "> я рад что ты пришел меня проведать .\n",
            "= i m glad you came to see me .\n",
            "< i m glad you were here than me . <EOS>\n",
            "\n",
            "> я не программист .\n",
            "= i m not a programmer .\n",
            "< i m not going . <EOS>\n",
            "\n",
            "> я не достоин .\n",
            "= i m not worthy .\n",
            "< i m not a . <EOS>\n",
            "\n",
            "> я сеичас иду в гостиницу .\n",
            "= i m going to the hotel now .\n",
            "< i m going to a school now . <EOS>\n",
            "\n",
            "> его поезд ушел .\n",
            "= he s missed the boat .\n",
            "< he s missed the . . <EOS>\n",
            "\n",
            "> мы просто ищем тома .\n",
            "= we re just looking for tom .\n",
            "< we re just looking for tom . . <EOS>\n",
            "\n",
            "> уверен что том вам поможет .\n",
            "= i m sure tom will help you .\n",
            "< i m sure tom will help you . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 слой + LSTM"
      ],
      "metadata": {
        "id": "nkjUfUNmysTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "rnn_type = nn.LSTM #nn.LSTM, nn.GRU\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers, rnn_type).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers, rnn_type).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75000, rnn_type, num_layers, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNtRmCKGy0iJ",
        "outputId": "5938a9eb-3e1e-4e12-fa34-1d0b7b91e41e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 20s (- 18m 43s) (5000 6%) 3.2157\n",
            "2m 38s (- 17m 7s) (10000 13%) 2.7744\n",
            "3m 56s (- 15m 46s) (15000 20%) 2.5621\n",
            "5m 15s (- 14m 27s) (20000 26%) 2.3949\n",
            "6m 35s (- 13m 10s) (25000 33%) 2.2522\n",
            "7m 54s (- 11m 51s) (30000 40%) 2.1148\n",
            "9m 14s (- 10m 33s) (35000 46%) 1.9867\n",
            "10m 33s (- 9m 14s) (40000 53%) 1.8910\n",
            "11m 53s (- 7m 55s) (45000 60%) 1.8070\n",
            "13m 12s (- 6m 36s) (50000 66%) 1.7681\n",
            "14m 33s (- 5m 17s) (55000 73%) 1.6669\n",
            "15m 52s (- 3m 58s) (60000 80%) 1.5822\n",
            "17m 12s (- 2m 38s) (65000 86%) 1.5450\n",
            "18m 32s (- 1m 19s) (70000 93%) 1.4507\n",
            "19m 51s (- 0m 0s) (75000 100%) 1.4202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, num_layers, rnn_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED3UNGp6y0UQ",
        "outputId": "5c6dc593-7143-48ad-8400-b51cc03372c6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> я начинаю думать что ты прав .\n",
            "= i m beginning to think you re right .\n",
            "< i m beginning to think you re right . <EOS>\n",
            "\n",
            "> я сыт по горло ее ворчанием .\n",
            "= i m fed up with her grumbling .\n",
            "< i m sick and tired of her . . <EOS>\n",
            "\n",
            "> я жду что том победит .\n",
            "= i m expecting tom to win .\n",
            "< i m waiting for tom to come . <EOS>\n",
            "\n",
            "> вы пессимист .\n",
            "= you re pessimistic .\n",
            "< you re pessimistic . <EOS>\n",
            "\n",
            "> вы очень смелая .\n",
            "= you re very brave .\n",
            "< you are very ambitious . <EOS>\n",
            "\n",
            "> я благодарен за все что у меня есть .\n",
            "= i m grateful for everything i have .\n",
            "< i m grateful i have everything . <EOS>\n",
            "\n",
            "> ты непоследователен .\n",
            "= you re inconsistent .\n",
            "< you re getting . <EOS>\n",
            "\n",
            "> на нее иногда находит депрессия .\n",
            "= she sometimes gets depressed .\n",
            "< she s on the the . . <EOS>\n",
            "\n",
            "> я пришел домои пораньше .\n",
            "= i m home early .\n",
            "< i m a lot of . . <EOS>\n",
            "\n",
            "> он типичныи японец .\n",
            "= he s a typical japanese man .\n",
            "< he s a teacher of . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 слоя + LSTM"
      ],
      "metadata": {
        "id": "n5N8DYQXywbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "rnn_type = nn.LSTM #nn.LSTM, nn.GRU\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers, rnn_type).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers, rnn_type).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75000, rnn_type, num_layers, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJMo3aGDyMhU",
        "outputId": "90457ec1-9c3f-45c4-e218-fb5b665fa0bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 44s (- 24m 23s) (5000 6%) 3.3266\n",
            "3m 27s (- 22m 26s) (10000 13%) 2.8889\n",
            "5m 10s (- 20m 43s) (15000 20%) 2.6745\n",
            "6m 54s (- 19m 0s) (20000 26%) 2.5363\n",
            "8m 38s (- 17m 16s) (25000 33%) 2.3819\n",
            "10m 23s (- 15m 34s) (30000 40%) 2.2676\n",
            "12m 6s (- 13m 50s) (35000 46%) 2.1205\n",
            "13m 50s (- 12m 6s) (40000 53%) 2.0603\n",
            "15m 34s (- 10m 23s) (45000 60%) 1.9377\n",
            "17m 18s (- 8m 39s) (50000 66%) 1.8492\n",
            "19m 3s (- 6m 55s) (55000 73%) 1.7541\n",
            "20m 47s (- 5m 11s) (60000 80%) 1.6955\n",
            "22m 32s (- 3m 28s) (65000 86%) 1.6374\n",
            "24m 16s (- 1m 44s) (70000 93%) 1.5655\n",
            "26m 0s (- 0m 0s) (75000 100%) 1.4901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, num_layers, rnn_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFhzHD5Ly02w",
        "outputId": "fa198602-b5e5-4b11-b451-74cc1061f14c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> мы одержим победу .\n",
            "= we re going to win .\n",
            "< we re going to . . <EOS>\n",
            "\n",
            "> мы встали рано .\n",
            "= we re up early .\n",
            "< we re getting early . <EOS>\n",
            "\n",
            "> они мертвые .\n",
            "= they are dead .\n",
            "< they are getting . <EOS>\n",
            "\n",
            "> вы чересчур вежливы .\n",
            "= you re too polite .\n",
            "< you re too good . <EOS>\n",
            "\n",
            "> я сын тома .\n",
            "= i m tom s son .\n",
            "< i m tom s brother . <EOS>\n",
            "\n",
            "> я почти готов идти .\n",
            "= i m about ready to go .\n",
            "< i m ready ready to go . <EOS>\n",
            "\n",
            "> я чудесно провожу время .\n",
            "= i m having an awesome time .\n",
            "< i m a a time time . <EOS>\n",
            "\n",
            "> ты сексист .\n",
            "= you re a sexist .\n",
            "< you re a little liar . <EOS>\n",
            "\n",
            "> я жду что мне кто нибудь поможет .\n",
            "= i m waiting for someone to help me .\n",
            "< i m waiting for a to answer here . <EOS>\n",
            "\n",
            "> мы не заинтересованы .\n",
            "= we re not interested .\n",
            "< we re not . . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы:\n",
        "1 слой + GRU: loss = 1.2043, time = 18 minute.  \n",
        "2 слоя + GRU: loss = 1.2130, time = 24 minute.  \n",
        "1 слой + LSTM: loss = 1.4202, time = 20 minute.  \n",
        "2 слоя + LSTM: loss = 1.4901, time = 26 minute.\n",
        "\n",
        "\n",
        "Функция потерь меньше всего в случае использования GRU и эта модель также обучается быстрее, в обоих кейсах увеличение числа слоев не привело к увеличению качества, но время обучения выросло."
      ],
      "metadata": {
        "id": "M97OsUh3zClP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnbxtNcHzFvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}